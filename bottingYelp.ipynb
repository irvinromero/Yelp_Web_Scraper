{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Page Counter as pagRev \n",
    "### Assign Search URL to website we will be scraping\n",
    "### Finally we will initalize dictionary to create structure of our dataset for the data we scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagRev = 0\n",
    "\n",
    "search_url ='https://www.yelp.com/biz/golden-gate-park-san-francisco'\n",
    "\n",
    "finalDataset = {\n",
    "        'Name': [],\n",
    "        'isElite': [],\n",
    "        'bizRating': [],\n",
    "        'dateReviewed': [],\n",
    "        'BusName' : [],\n",
    "        'BusCategory' : [],\n",
    "        'BusReviewCounts' : [],\n",
    "        'ParkingInfo': []\n",
    "}    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create functions to easily scrape and export our data\n",
    "\n",
    " - __get_bus_meta:__  function that will scrape and return business name, business categories, '__Known for__' details, and Total business review counts  \n",
    " - __get_yelp_data( yelpObj ):__ function that takes in beautifulSoup object and grabs reviewer data. This includes reviewer user name, user elite status, reviewer rating between 1-5, and date when review was written  \n",
    " - __parse_page( next_url, pagin ):__ function that takes in a url to search and page counter. First, we set the maxium number of pages we would like to scrape from each business. This function will call our __get_yelp_data__ function to extract review data and then continue for each page until we reach last page or the maximum number of pages set  \n",
    " - __export_table_and_print( data ):__ function that takes in dictionary. This will transform our dictionary to a dataset and we will extract as a .csv file ***Make sure to rename csv file***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_bus_meta(urlSame):\n",
    "\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(urlSame)\n",
    "    driver.implicitly_wait(100)\n",
    "    tutorial_soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    \n",
    "    # Create Values to return\n",
    "    \n",
    "    placeName = \"\"\n",
    "    busReviewCount = \"\"\n",
    "    busCats = \"\"\n",
    "    testPark = {}\n",
    "    parkingVal = []\n",
    "    parkingKey = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### Grab business Name\n",
    "    grabBusName = tutorial_soup.findAll('div', attrs={'class': 'lemon--div__373c0__1mboc u-space-b1 border-color--default__373c0__2oFDT'})\n",
    "    placeName = grabBusName[0].find('h1').text\n",
    "    \n",
    "    # grab categories\n",
    "    grabCats = tutorial_soup.findAll('span',attrs={'class':'lemon--span__373c0__3997G display--inline__373c0__1DbOG u-space-r1 border-color--default__373c0__2oFDT'})\n",
    "    busCats = grabCats[1].text\n",
    "\n",
    "    \n",
    "    # grab total bus reviews\n",
    "    totalRevs = tutorial_soup.findAll('p',attrs={'class':'lemon--p__373c0__3Qnnj text__373c0__2pB8f text-color--mid__373c0__3G312 text-align--left__373c0__2pnx_ text-size--large__373c0__1568g'})\n",
    "    print(totalRevs[0].text)\n",
    "    busReviewCount = totalRevs[0].text\n",
    "\n",
    "    \n",
    "    tts = tutorial_soup.findAll('span', attrs={'class':'lemon--span__373c0__3997G text__373c0__2pB8f text-color--normal__373c0__K_MKN text-align--left__373c0__2pnx_'})\n",
    "    print('key 1')\n",
    "    print(tts[9].text)\n",
    "    parkingKey.append(tts[9].text)\n",
    "    print('key 2')    \n",
    "    print(tts[10].text)    \n",
    "    parkingKey.append(tts[10].text)    \n",
    "    print('key 3')    \n",
    "    print(tts[11].text) \n",
    "    parkingKey.append(tts[11].text)    \n",
    "    print('key 4')    \n",
    "    print(tts[12].text)        \n",
    "    parkingKey.append(tts[12].text)\n",
    "    \n",
    "\n",
    "    ciclr =0\n",
    "    tte = tutorial_soup.findAll('span', attrs={'class':'lemon--span__373c0__3997G text__373c0__2pB8f text-color--normal__373c0__K_MKN text-align--left__373c0__2pnx_ text-weight--bold__373c0__3HYJa'})\n",
    "    for test123 in tte:\n",
    "        ciclr = ciclr +1\n",
    "        print(ciclr)\n",
    "        print(test123.text)\n",
    "    strSZ1 = len(tte[7].text)\n",
    "    print(tte[7].text)\n",
    "    parkingVal.append(tte[7].text[1:strSZ1])\n",
    "    strSZ2 = len(tte[8].text)\n",
    "    print(tte[8].text)\n",
    "    parkingVal.append(tte[8].text[1:strSZ2])\n",
    "    strSZ3 = len(tte[9].text) \n",
    "    print(tte[9].text)\n",
    "    parkingVal.append(tte[9].text[1:strSZ3])\n",
    "    strSZ4 = len(tte[10].text)\n",
    "    print(tte[10].text)\n",
    "    parkingVal.append(tte[10].text[1:strSZ4])\n",
    "\n",
    "    \n",
    "    for key in parkingKey: \n",
    "        for value in parkingVal: \n",
    "            testPark[key] = value \n",
    "            parkingVal.remove(value) \n",
    "            break\n",
    "\n",
    "    # return values created\n",
    "    return (placeName, busCats,testPark, busReviewCount)\n",
    "\n",
    "\n",
    "\n",
    "def get_yelp_data(yelpObj):\n",
    "    \n",
    "    \n",
    "    \n",
    "    tew = yelpObj.findAll('div', attrs={'class': 'lemon--div__373c0__1mboc arrange__373c0__UHqhV gutter-6__373c0__zqA5A vertical-align-middle__373c0__2TQsQ border-color--default__373c0__2oFDT'})\n",
    "    print('whole obj len')\n",
    "    print(len(tew))\n",
    "    for t in tew:\n",
    "        testDate = t.find('span', attrs={'class': 'lemon--span__373c0__3997G text__373c0__2pB8f text-color--mid__373c0__3G312 text-align--left__373c0__2pnx_'})\n",
    "        testStar = t.findAll('span', attrs={'class':'lemon--span__373c0__3997G display--inline__373c0__1DbOG border-color--default__373c0__2oFDT'})\n",
    "        notUpdated = t.findAll('div', attrs= {'class':'lemon--div__373c0__1mboc display--inline-block__373c0__2de_K u-space-r-half border-color--default__373c0__2oFDT'})\n",
    "        updat = t.findAll('span' , attrs = {'class':'lemon--span__373c0__3997G text__373c0__2pB8f text-color--mid__373c0__3G312 text-align--left__373c0__2pnx_ text-weight--bold__373c0__3HYJa text-size--small__373c0__3SGMi'})\n",
    "\n",
    "        \n",
    "        if (len(updat)==1 and len(notUpdated)==1):\n",
    "            finalDataset['dateReviewed'].append(testDate.text)\n",
    "            finalDataset['bizRating'].append(testStar[0].find('div')['aria-label'][0])\n",
    "        \n",
    "        \n",
    "        if(testDate and len(updat)==0 and len(notUpdated)==0):\n",
    "            #print(testDate.text)\n",
    "            print(notUpdated)\n",
    "            finalDataset['dateReviewed'].append(testDate.text)\n",
    "\n",
    "\n",
    "        if(testStar and len(updat)==0) and len(notUpdated)==0:\n",
    "            print(testStar[0].find('div')['aria-label'])\n",
    "            print(type(testStar[0].find('div')['aria-label']))\n",
    "            finalDataset['bizRating'].append(testStar[0].find('div')['aria-label'][0])\n",
    "    \n",
    "\n",
    "             \n",
    "    # grab user elite status and username        \n",
    "   \n",
    "    findusr = yelpObj.findAll('div', attrs={'class':'lemon--div__373c0__1mboc user-passport-info border-color--default__373c0__2oFDT'})\n",
    "    for usaInfo in findusr:\n",
    "        #print(usaInfo.find('a').text)\n",
    "        finalDataset['Name'].append(usaInfo.find('a').text)\n",
    "    \n",
    "    findElite = yelpObj.findAll('div', attrs={'class':'lemon--div__373c0__1mboc user-passport-stats__373c0__2LjLz border-color--default__373c0__2oFDT'})\n",
    "\n",
    "    for eliInfo in findElite:\n",
    "        if (eliInfo.find('a')):\n",
    "            numOnly = eliInfo.find('a').text\n",
    "            finalDataset['isElite'].append(numOnly.replace(\"Elite '\",''))\n",
    "            #print(numOnly.replace(\"Elite '\",''))\n",
    "        else: \n",
    "            #print(0)\n",
    "            finalDataset['isElite'].append(0)\n",
    " \n",
    "\n",
    "    print(finalDataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_page(next_url, pagin):\n",
    "    maxPg = 50\n",
    "    \n",
    "    # HTTP GET requests\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(next_url)\n",
    "    driver.implicitly_wait(100)\n",
    "      \n",
    "    pages_remaining = True\n",
    " \n",
    "    while pages_remaining:\n",
    "    # Extract reviews and check if theres a next page\n",
    "        bs = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        \n",
    "        get_yelp_data(bs)\n",
    "        pagin = pagin + 1\n",
    "        try:\n",
    "            #check for max pages\n",
    "            if(pagin > maxPg):\n",
    "                print('Max Page reached!')\n",
    "                pages_remaining = False\n",
    "            #Checks if there are more pages with links\n",
    "            next_link  = driver.find_element_by_link_text('Next')\n",
    "            next_link.click()\n",
    "            time.sleep(5)\n",
    "\n",
    "        except (NoSuchElementException):\n",
    "            print('Last Page Reached!')\n",
    "            pages_remaining = False\n",
    "            driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "def export_table_and_print(data):\n",
    "    table = pd.DataFrame(data, columns=['Name','isElite','bizRating','dateReviewed',\n",
    "                                        'BusName','BusCategory', 'BusReviewCounts', 'ParkingInfo'])\n",
    "    table.index = table.index + 1\n",
    "    # Print to see what we've created\n",
    "    print(table)\n",
    "    table.to_csv('yelpData.csv', sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin business data extraction\n",
    "\n",
    "Since our __get_bus_meta__ function has data extracted in an array format we append it to our dataset for n amount of rows in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_page(search_url, pagRev)\n",
    "x = get_bus_meta(search_url)\n",
    "\n",
    "szOfDtf = len(finalDataset['Name'])\n",
    "print(len(x))\n",
    "for i in range(szOfDtf):\n",
    "    finalDataset['BusName'].append(x[0])\n",
    "    finalDataset['BusCategory'].append(x[1])\n",
    "    finalDataset['ParkingInfo'].append(x[2])\n",
    "    finalDataset['BusReviewCounts'].append(x[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally we extract our dataset to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "export_table_and_print(finalDataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
